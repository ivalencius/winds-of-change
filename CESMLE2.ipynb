{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "71754fc0-3ed1-4ea9-813a-b825b0858440",
   "metadata": {},
   "source": [
    "# CESM2 Large Ensemble 2 Investiagtion\n",
    "\n",
    "Daily files are located under `/glade/campaign/cgd/cesm/CESM2-LE/timeseries/atm/proc/tseries/day_1/WSPDSRFAV/*` on the NCAR server.\n",
    "\n",
    "__Variables analyzed__\n",
    "- `WSPDSRFAV`: Horizontal total wind speed average at the surface [$m \\ s^{-1}$]\n",
    "- _WSPDSRFAV anomaly_: artificially constructed according to $x_i-\\bar{x}_{\\text{time}}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e728276b-26ac-42f7-9446-61bc2808f2d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from matplotlib import colors\n",
    "import cartopy.crs as ccrs\n",
    "import cartopy.feature as cfeature\n",
    "from cartopy.mpl.ticker import LatitudeFormatter, LongitudeFormatter\n",
    "import nc_time_axis\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import pwlf\n",
    "import xarray as xr\n",
    "import cf_xarray as cfxr   # to use cf_xarray attributes\n",
    "import regionmask\n",
    "from glob import glob\n",
    "import scienceplots\n",
    "%matplotlib inline\n",
    "plt.style.use([\"nature\", \"notebook\"])\n",
    "\n",
    "xr.set_options(keep_attrs=True)\n",
    "%load_ext rich\n",
    "from rich import print  # pretty printing\n",
    "from tqdm import tqdm  # progress bar\n",
    "import warnings  # deal with warnings\n",
    "\n",
    "# To access collection\n",
    "import dask\n",
    "import intake\n",
    "from dask_jobqueue import PBSCluster\n",
    "from dask.distributed import Client\n",
    "\n",
    "# Play nice with CMIP6 data\n",
    "import xclim.ensembles as ensembles"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2692e3b-fcab-49f1-87f2-77b112e34781",
   "metadata": {},
   "source": [
    "## Helper functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "322e8583-8fbf-4e77-a74d-2dc0d297c73e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def mask_data(data, map, regions: None, drop=False, reverse=False):\n",
    "    \"\"\"Mask xarray data based on region names\n",
    "\n",
    "    Args:\n",
    "        data (xarray dataset): xarray dataset to mask\n",
    "        map (regionmask): regionmask object\n",
    "        regions (list or None): list of region names to mask. if None, all regions are taken. Defaults to None.\n",
    "        drop (bool, optional): Whether to drop when masking. Defaults to False.\n",
    "        reverse (bool, optional): Whether to mask the inverse of the regions. Defaults to False.\n",
    "\n",
    "    Returns:\n",
    "        xarray dataset: masked dataset\n",
    "    \"\"\"\n",
    "    # Load the region mask\n",
    "    mask = map.mask(data.cf['lat'], data.cf['lon'])\n",
    "    # Extract keys for the region\n",
    "    id_dict = map.region_ids\n",
    "    # Get region names\n",
    "    if regions is None:\n",
    "        names = id_dict.keys()\n",
    "    else:\n",
    "        # Coerce region names to uppercase\n",
    "        regions = [region.upper() for region in regions]\n",
    "        names = [name for name in id_dict.keys() if str(name).upper() in regions]\n",
    "        assert len(names) == len(regions), 'Not enough regions found'\n",
    "    # Get the key for the regions\n",
    "    keys = [id_dict[name] for name in names]\n",
    "    # Apply the mask to the data\n",
    "    if reverse:\n",
    "        masked_data = data.where(~mask.isin(keys), drop=drop)\n",
    "    else:\n",
    "        masked_data = data.where(mask.isin(keys), drop=drop)\n",
    "    return masked_data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b1ffc36-e227-499d-834f-df2d268784d1",
   "metadata": {},
   "source": [
    "## Spin up Dask cluster"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ebe9926-d4fd-4f09-8870-8ba36d7b0eac",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create our NCAR Cluster - which uses PBSCluster under the hood\n",
    "num_jobs = 5\n",
    "cluster = PBSCluster(\n",
    "    job_name = 'valencig-dask-hpc',\n",
    "    cores = 4,\n",
    "    memory = '10GiB',\n",
    "    local_directory = '/glade/u/home/valencig/spilled/',\n",
    "    log_directory = '/glade/u/home/valencig/worker-logs/',\n",
    "    queue = 'casper',\n",
    "    walltime = '02:00:00', # Change wall time if needed\n",
    "    interface = 'ext'\n",
    ")\n",
    "\n",
    "\n",
    "# Spin up workers\n",
    "cluster.scale(jobs=num_jobs)\n",
    "\n",
    "# Assign the cluster to our Client\n",
    "client = Client(cluster)\n",
    "\n",
    "# Block progress until workers have spawned\n",
    "client.wait_for_workers(num_jobs)\n",
    "client"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15267b38-a677-4940-96a1-52ccb8029923",
   "metadata": {},
   "source": [
    "## Create ensemble"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30aa1609-2525-4c50-a6fe-3fdb09489546",
   "metadata": {},
   "source": [
    "### Get simulation names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9bd33f06-55c3-4d61-8503-c1599a9397e2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"></pre>\n"
      ],
      "text/plain": []
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "\u001b[1m{\u001b[0m\u001b[32m'BHISTcmip6'\u001b[0m, \u001b[32m'BSSP370cmip6'\u001b[0m, \u001b[32m'BSSP370smbb'\u001b[0m, \u001b[32m'BHISTsmbb'\u001b[0m\u001b[1m}\u001b[0m"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Get list of all files\n",
    "all_files = glob('/glade/campaign/cgd/cesm/CESM2-LE/timeseries/atm/proc/tseries/day_1/WSPDSRFAV/*')\n",
    "# Simulation runs\n",
    "simulations = set([f.split('.')[2] for f in all_files])\n",
    "simulations"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "977f72d6-4801-4bcc-b1ec-b76befa61921",
   "metadata": {},
   "source": [
    "### Load one simulation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "edfd88ff-af2e-47d4-81a8-5e3e34f3bd0d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading members...: 100%|██████████| 14/14 [01:13<00:00,  5.26s/it]\n"
     ]
    }
   ],
   "source": [
    "# Extract all member numbers\n",
    "sim_files = [f for f in all_files if f.split('.')[2] == 'BSSP370smbb']\n",
    "members = list(set([f.split('.')[4] for f in sim_files]))\n",
    "# Create list to store datasets for each member\n",
    "sim_list = []\n",
    "for member in tqdm(members, desc=\"Loading members...\"):\n",
    "    # Get files for this member\n",
    "    m_files = [f for f in sim_files if f.split('.')[4] == member]\n",
    "    # Load dataset\n",
    "    ds = xr.open_mfdataset(m_files)\n",
    "    # Remove 2100 (data extends to 2099)\n",
    "    sliced = ds.sel(time=slice(None, '2099'))\n",
    "    # Expand dimension name\n",
    "    sim_list.append(sliced.WSPDSRFAV)\n",
    "simulation = ensembles.create_ensemble(sim_list, realizations=members)\n",
    "yearly = simulation.groupby('time.year').mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eaafcbf3-6a65-4eff-a58c-3fe865557261",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Masking regions\n",
    "land_region = regionmask.defined_regions.natural_earth_v5_0_0.land_110  # Land has value 0\n",
    "countries = regionmask.defined_regions.natural_earth_v5_0_0.countries_110\n",
    "\n",
    "# mask to land\n",
    "land = mask_data(yearly, land_region, ['land'], drop=True)\n",
    "# Eliminate the planet of Hoth\n",
    "land = mask_data(land, countries, ['greenland'], reverse=True, drop=True)\n",
    "land = land.where((land['lat']>-59).compute(), drop=True)  # antarctica\n",
    "land = land.where((land['lat']<70).compute(), drop=True)  # Northern canada\n",
    "land = land.compute()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9936e3c-c28a-4cba-925f-f41af51ef611",
   "metadata": {},
   "outputs": [],
   "source": [
    "land['WSPDSRFAV'].mean(['lat', 'lon']).plot(hue='realization')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "caffd304-1147-434e-b767-7c065edc2116",
   "metadata": {},
   "source": [
    "## Kill Dask Cluster"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f90292bc-44cd-4288-8c79-c04a2b1a094d",
   "metadata": {},
   "outputs": [],
   "source": [
    "client.shutdown()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:.conda-wind]",
   "language": "python",
   "name": "conda-env-.conda-wind-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
