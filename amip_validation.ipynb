{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "bcf8ce99",
   "metadata": {},
   "source": [
    "# Atmospheric Model Intercomparison Project (AMIP) Validation\n",
    "\n",
    "To investigate global stilling, we want to force the models realistic historical SST forcing in order to force the models with the observed realization. To do this we use the `amip` model runs.\n",
    "\n",
    "__Variables analyzed__\n",
    "<!-- - `uas`: eastward wind component (usually 10 m) [$m \\ s^{-1}$]\n",
    "- `vas`: northward wind component (usually 10 m) [$m \\ s^{-1}$] -->\n",
    "- `sfcWind`: Near-Surface Wind Speed [$m \\ s^{-1}$]\n",
    "- _sfcWind anomaly_: artificially constructed according to $x_i-\\bar{x}_{\\text{time}}$\n",
    "\n",
    "Models are available on Andromeda (The BC Cluster) at `/data/projects/bccg/CMIP6/amip/mon/sfcWind`. Models are at _monthly_ resolution and aggregated _yearly_ before any trend analysis is analyzed.\n",
    "\n",
    "__Steps to connect to BC Cluster__\n",
    "1. Install Remote SSH and Remote X11 extensions in VScode\n",
    "2. `ssh -Y username@andromeda.bc.edu`\n",
    "3. Enter password\n",
    "4. You are now in your home directory located at `~/mmfs1/data/_username_`\n",
    "5. `module load python/3.9.0` $\\leftarrow$ add to .tcshrc file to automatically load every login\n",
    "\n",
    "__To start an interactive session__:\n",
    "\n",
    "`interactive -t [DD-hh:mm] [-N nodes) [-n tasks] [-c cpus-per-task] [-m gb] [-p partition] [-G #] [-X] [-h]`\n",
    "\n",
    "Options:  \n",
    "- `t`: Wall Time (default is 4 hours)\n",
    "- `N`: Number of nodes (default is 1) \n",
    "- `m`: GB of Memory per node (default is 4GB) \n",
    "- `n`: Number of tasks per node (default is 1) \n",
    "- `c`: Number of cpu cores per task (default is 4) \n",
    "- `X`: Use X11 \n",
    "- `p` <partition name>: Use the partition specified (default is shared) \n",
    "- `G` #: Specify the number of GPUs per gpu node\n",
    "- `h`: help\n",
    "\n",
    "My default command: `interactive`\n",
    "\n",
    "Useful command to monitor usage: `htop`\n",
    "\n",
    "__Note__: This will not allow you to run a Jupyter Notebook interactively, only scripts. To run notebooks interactively use `dask_jobqueue.SLURMCluster()`\n",
    "\n",
    "__Getting Conda up and Running__\n",
    "1. `module load anaconda/2023.07-p3.11`\n",
    "2. `conda init tcsh`\n",
    "3. `conda create -n _envname_ python=3.11`\n",
    "4. `conda activate _envname_`\n",
    "\n",
    "This will create a conda environment in the `/mmfs1/data/_username_/.conda/envs/_envname_` directory. \n",
    "\n",
    "To automatically use this environment on login use add `conda activate _envname_` to your `.tcshrc` file.\n",
    "\n",
    "__For a faster environment solver__\n",
    "1. `conda install -n _envname_ conda-libmamba-solver`\n",
    "2. `conda config --set solver libmamba`\n",
    "\n",
    "<!-- __Export Environment__: `conda env export --no_builds > environment.yml` -->\n",
    "\n",
    "__Note__: To use `matplotlib` we must install $\\LaTeX$. Jupyter Notebooks use MathJax under the hood which is why we only need to install if it using $\\LaTeX$ in python.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "6ce0c44c-9012-4577-8a14-b27825e7e965",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The rich extension is already loaded. To reload it, use:\n",
      "  %reload_ext rich\n"
     ]
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from matplotlib import colors\n",
    "import cartopy.crs as ccrs\n",
    "import cartopy.feature as cfeature\n",
    "from cartopy.mpl.ticker import LatitudeFormatter, LongitudeFormatter\n",
    "import nc_time_axis\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import pwlf\n",
    "import xarray as xr\n",
    "import cf_xarray as cfxr   # to use cf_xarray attributes\n",
    "import regionmask\n",
    "from glob import glob\n",
    "import scienceplots\n",
    "%matplotlib inline\n",
    "plt.style.use([\"nature\", \"notebook\"])\n",
    "\n",
    "xr.set_options(keep_attrs=True)\n",
    "%load_ext rich\n",
    "from rich import print  # pretty printing\n",
    "from tqdm import tqdm  # progress bar\n",
    "import warnings  # deal with warnings\n",
    "\n",
    "# Playing nice with CMIP6\n",
    "# from xmip.preprocessing import combined_preprocessing\n",
    "# from xclim.ensembles import create_ensemble, ensemble_mean_std_max_min"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4ad408b",
   "metadata": {},
   "source": [
    "## Spin up dask cluster"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "0582ae03",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #800000; text-decoration-color: #800000\">╭─────────────────────────────── </span><span style=\"color: #800000; text-decoration-color: #800000; font-weight: bold\">Traceback </span><span style=\"color: #bf7f7f; text-decoration-color: #bf7f7f; font-weight: bold\">(most recent call last)</span><span style=\"color: #800000; text-decoration-color: #800000\"> ────────────────────────────────╮</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span> in <span style=\"color: #00ff00; text-decoration-color: #00ff00\">&lt;module&gt;</span>:<span style=\"color: #0000ff; text-decoration-color: #0000ff\">5</span>                                                                                    <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>                                                                                                  <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"> 2 </span>num_jobs = <span style=\"color: #0000ff; text-decoration-color: #0000ff\">3</span>                                                                                <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"> 3 </span>walltime = <span style=\"color: #808000; text-decoration-color: #808000\">'02:00:00'</span>                                                                       <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"> 4 </span>                                                                                            <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span> <span style=\"color: #800000; text-decoration-color: #800000\">❱ </span> 5 cluster = SLURMCluster(                                                                     <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"> 6 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   </span>memory=<span style=\"color: #808000; text-decoration-color: #808000\">\"8g\"</span>,                                                                            <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"> 7 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   </span>processes=<span style=\"color: #0000ff; text-decoration-color: #0000ff\">1</span>,                                                                            <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"> 8 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   </span>cores=<span style=\"color: #0000ff; text-decoration-color: #0000ff\">4</span>,                                                                                <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">╰──────────────────────────────────────────────────────────────────────────────────────────────────╯</span>\n",
       "<span style=\"color: #ff0000; text-decoration-color: #ff0000; font-weight: bold\">NameError: </span>name <span style=\"color: #008000; text-decoration-color: #008000\">'SLURMCluster'</span> is not defined\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[31m╭─\u001b[0m\u001b[31m──────────────────────────────\u001b[0m\u001b[31m \u001b[0m\u001b[1;31mTraceback \u001b[0m\u001b[1;2;31m(most recent call last)\u001b[0m\u001b[31m \u001b[0m\u001b[31m───────────────────────────────\u001b[0m\u001b[31m─╮\u001b[0m\n",
       "\u001b[31m│\u001b[0m in \u001b[92m<module>\u001b[0m:\u001b[94m5\u001b[0m                                                                                    \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m                                                                                                  \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m 2 \u001b[0mnum_jobs = \u001b[94m3\u001b[0m                                                                                \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m 3 \u001b[0mwalltime = \u001b[33m'\u001b[0m\u001b[33m02:00:00\u001b[0m\u001b[33m'\u001b[0m                                                                       \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m 4 \u001b[0m                                                                                            \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m \u001b[31m❱ \u001b[0m 5 cluster = SLURMCluster(                                                                     \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m 6 \u001b[0m\u001b[2m│   \u001b[0mmemory=\u001b[33m\"\u001b[0m\u001b[33m8g\u001b[0m\u001b[33m\"\u001b[0m,                                                                            \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m 7 \u001b[0m\u001b[2m│   \u001b[0mprocesses=\u001b[94m1\u001b[0m,                                                                            \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m 8 \u001b[0m\u001b[2m│   \u001b[0mcores=\u001b[94m4\u001b[0m,                                                                                \u001b[31m│\u001b[0m\n",
       "\u001b[31m╰──────────────────────────────────────────────────────────────────────────────────────────────────╯\u001b[0m\n",
       "\u001b[1;91mNameError: \u001b[0mname \u001b[32m'SLURMCluster'\u001b[0m is not defined\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# For using dask cluster\n",
    "# from dask_jobqueue import SLURMCluster\n",
    "# from distributed import Client\n",
    "\n",
    "# Change params as needed\n",
    "# num_jobs = 3\n",
    "# walltime = '02:00:00'\n",
    "\n",
    "# cluster = SLURMCluster(\n",
    "#     memory=\"8g\",\n",
    "#     processes=1,\n",
    "#     cores=4, \n",
    "#     log_directory='/mmfs1/data/valencig/worker-logs/',\n",
    "#     walltime=walltime,\n",
    "# )\n",
    "\n",
    "# # Assign the cluster to our Client\n",
    "# client = Client(cluster)\n",
    "\n",
    "# # Block progress until workers have spawned\n",
    "# client.wait_for_workers(num_jobs)\n",
    "\n",
    "# client"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0818abd5",
   "metadata": {},
   "source": [
    "## Helper Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3aca39c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def mask_data(data, map, regions: None, drop=False, reverse=False):\n",
    "    \"\"\"Mask xarray data based on region names\n",
    "\n",
    "    Args:\n",
    "        data (xarray dataset): xarray dataset to mask\n",
    "        map (regionmask): regionmask object\n",
    "        regions (list or None): list of region names to mask. if None, all regions are taken. Defaults to None.\n",
    "        drop (bool, optional): Whether to drop when masking. Defaults to False.\n",
    "        reverse (bool, optional): Whether to mask the inverse of the regions. Defaults to False.\n",
    "\n",
    "    Returns:\n",
    "        xarray dataset: masked dataset\n",
    "    \"\"\"\n",
    "    # Load the region mask\n",
    "    mask = map.mask(data.cf['X'], data.cf['Y'])\n",
    "    # Extract keys for the region\n",
    "    id_dict = map.region_ids\n",
    "    # Get region names\n",
    "    if regions is None:\n",
    "        names = id_dict.keys()\n",
    "    else:\n",
    "        # Coerce region names to uppercase\n",
    "        regions = [region.upper() for region in regions]\n",
    "        names = [name for name in id_dict.keys() if str(name).upper() in regions]\n",
    "        assert len(names) == len(regions), 'Not enough regions found'\n",
    "    # Get the key for the regions\n",
    "    keys = [id_dict[name] for name in names]\n",
    "    # Apply the mask to the data\n",
    "    if reverse:\n",
    "        masked_data = data.where(~mask.isin(keys), drop=drop)\n",
    "    else:\n",
    "        masked_data = data.where(mask.isin(keys), drop=drop)\n",
    "    return masked_data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40943d86",
   "metadata": {},
   "source": [
    "## Create the ensemble"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "500a45c7",
   "metadata": {},
   "source": [
    "### Model names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "761ef6a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get all model names\n",
    "model_folders = glob('/Volumes/valencig@bc/data/amip/mon/sfcWind/*')\n",
    "model_names = [f.split('/')[-1] for f in model_folders]\n",
    "model_names = sorted(model_names)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b504cf3c",
   "metadata": {},
   "source": [
    "### Remove certain models\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61298cc2",
   "metadata": {},
   "source": [
    "Removal keys: \n",
    "- `__ km`: Nominal model resolution. We are looking for models of $\\leq$ 1 degrees (100 km or less).\n",
    "\n",
    "Data about models manually searched from [here](https://wcrp-cmip.github.io/CMIP6_CVs/docs/CMIP6_source_id.html)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96a59d6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "bad_models = [\n",
    "    'ACCESS-CM2',       # 250 km\n",
    "    'ACCESS-ESM1-5',    # 250 km\n",
    "    'BCC-ESM1',         # 250 km\n",
    "    'CESM2-FV2',        # 250 km\n",
    "    'CESM2-WACCM-FV2',  # 250 km\n",
    "    'CNRM-CM6-1',       # 250 km\n",
    "    'CNRM-ESM2-1',      # 250 km\n",
    "    'CanESM5',          # 500 km\n",
    "    'EC-Earth3-Veg-LR', # 250 km\n",
    "    'FGOALS-g3',        # 250 km\n",
    "    'GISS-E2-1-G',      # 250 km\n",
    "    'GISS-E2-2-G',      # 250 km\n",
    "    'HadGEM3-GC31-LL',  # 250 km\n",
    "    'ICON-ESM-LR',      # 250 km\n",
    "    'IITM-ESM',         # 250 km\n",
    "    'IPSL-CM6A-LR',     # 250 km\n",
    "    'KACE-1-0-G',       # 250 km\n",
    "    'KIOST-ESM',        # 250 km\n",
    "    'MIROC-ES2L',       # 500 km\n",
    "    'MIROC6',           # 250 km\n",
    "    'MPI-ESM-1-2-HAM',  # 250 km\n",
    "    'MPI-ESM1-2-LR',    # 250 km\n",
    "    'NESM3',            # 250 km\n",
    "    'NorESM2-LM',       # 250 km\n",
    "    'UKESM1-0-LL'       # 250 km\n",
    "]\n",
    "\n",
    "model_names = [model for model in model_names if model not in bad_models]\n",
    "n_models = len(model_names)\n",
    "print(f'{n_models} models found...')\n",
    "print('Model names:')\n",
    "model_names"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b148eaa",
   "metadata": {},
   "source": [
    "### Load datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8938594",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create list to hold datasets\n",
    "ensemble = dict()\n",
    "# Create dictionary to display indices of models\n",
    "realizations_dict = dict()\n",
    "with warnings.catch_warnings():  # suppress warnings from xarray bookeeping\n",
    "    warnings.simplefilter(\"ignore\")\n",
    "    for model in tqdm(model_names, desc='Loading models'):\n",
    "        # Sort to make sure realization indices increasing\n",
    "        paths = sorted(glob(f'/Volumes/valencig@bc/data/amip/mon/sfcWind/{model}/*'))\n",
    "        # Extract first realization\n",
    "        realization = paths[0].split('_')[-3]\n",
    "        # Add to dictionary\n",
    "        realizations_dict[model] = realization\n",
    "        # Filter paths to only include selected realization\n",
    "        realization_paths = [p for p in paths if realization in p]\n",
    "        \n",
    "        # Create ensembles\n",
    "        # ens = create_ensemble(paths, realizations=realizations)\n",
    "        # Combine realizations\n",
    "        # keep_vars = ['sfcWind_mean', 'sfcWind_stdev', 'sfcWind_max', 'sfcWind_min']\n",
    "        # reduced = ensemble_mean_std_max_min(ens)[keep_vars]\n",
    "        \n",
    "        # Load data\n",
    "        ds = xr.open_mfdataset(realization_paths, combine='by_coords', use_cftime=True)\n",
    "        # Convert the calendar to standard to merge across models\n",
    "        calendar_corrected = ds.convert_calendar('standard', align_on='year', use_cftime=True)\n",
    "        # Check times\n",
    "        sliced = calendar_corrected.sel(time=slice('1979', '2014'))\n",
    "        # Add to ensemble\n",
    "        ensemble[model] = sliced\n",
    "        \n",
    "    print(realizations_dict)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19c738cc",
   "metadata": {},
   "source": [
    "## Imports GSOD stations from [Zeng et. al (2019).](https://www.nature.com/articles/s41558-019-0622-6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84e9d757",
   "metadata": {},
   "outputs": [],
   "source": [
    "obs_path = 'data/Zeng-2019/41558_2019_622_MOESM2_ESM.xlsx'\n",
    "# GSOD data from Zeng (2019) -> not using HADISD\n",
    "excel = pd.ExcelFile(obs_path)\n",
    "# See sheet names\n",
    "stations = excel.parse('stations')\n",
    "obs_lats = stations['lats']\n",
    "obs_lons = stations['lons']  # don't want negative degrees\n",
    "station_nums = stations['stations']\n",
    "obs_winds = excel.parse('winds')\n",
    "# Construct years out of numpy\n",
    "years = np.arange(1979, 2015)\n",
    "\n",
    "gsod = xr.Dataset(\n",
    "            data_vars={\n",
    "                'GSOD': ([\"station\", \"year\"], obs_winds.to_numpy()[:, 2:-3]) \n",
    "                # First col is index, second col is 1978, last col is 2017\n",
    "            },\n",
    "            coords={\n",
    "                'station': station_nums,\n",
    "                # 'year': pd.date_range(start='1978', end='2015', freq='Y'), # [1978, 2014]\n",
    "                'year': years,\n",
    "                'lon': (\"station\", obs_lons),\n",
    "                'lat': (\"station\", obs_lats)\n",
    "            }\n",
    "    )\n",
    "# Get yearly data\n",
    "anom = gsod.GSOD - gsod.GSOD.mean('year')\n",
    "obs_anom = anom.groupby('year').mean('station') # means\n",
    "\n",
    "# Compute gsod piecewise linear fit\n",
    "gsod_pwlf = pwlf.PiecewiseLinFit(years, obs_anom)\n",
    "breaks = gsod_pwlf.fit(2)\n",
    "break_year = int(breaks[1])\n",
    "slopes = gsod_pwlf.calc_slopes()*10\n",
    "p_vals = gsod_pwlf.p_values(method='non-linear')[1:]  # betas are [intercept, slope1, slope2, ...]\n",
    "# print(f'Piecewise Parameters\\n\\tBreakpoint: {break_year}\\n\\tDecadal Slopes: {slopes}\\n\\tP-Values: {p_vals}')\n",
    "y_hat = gsod_pwlf.predict(years)\n",
    "\n",
    "# Make plot\n",
    "fig, ax = plt.subplots(figsize=(7, 5), nrows=1, ncols=1)\n",
    "ax_min = min(obs_anom) - 0.05\n",
    "ax_max = max(obs_anom) + 0.05\n",
    "ax.plot(years, obs_anom, linestyle='--', color='blue')\n",
    "ax.plot(years, y_hat, '-', color='k')\n",
    "ax.vlines(break_year, ax_min, ax_max, color='gray', linestyle='--')\n",
    "ax.set_ylim(ax_min, ax_max)\n",
    "ax.set_xlim(1978, 2014)\n",
    "# ax.fill_between(years, obs_q.isel(quantile=0), obs_q.isel(quantile=1), color='red', alpha=0.1)\n",
    "# Add small annotation\n",
    "ins = ax.inset_axes([0.7, 0.6, 0.25, 0.35])\n",
    "pos = slopes > 0\n",
    "neg = slopes < 0\n",
    "l = np.array([1, 2])\n",
    "for i, (m, p) in enumerate(zip(slopes, p_vals)):\n",
    "    i+=1\n",
    "    if m > 0:\n",
    "        rects = ins.bar(i, m, color='red')\n",
    "    else:\n",
    "        rects = ins.bar(i, m, color='blue')\n",
    "    if p < 0.01:\n",
    "        ins.bar_label(rects, ['p$<$0.01'])\n",
    "# Partition the percentile values to be able to draw large numbers in\n",
    "# white within the bar, and small numbers in black outside the bar.\n",
    "ins.hlines(0, 0.25, 2.75, color='k')\n",
    "# Remove spines\n",
    "ins.spines['top'].set_visible(False)\n",
    "ins.spines['right'].set_visible(False)\n",
    "ins.spines['bottom'].set_visible(False)\n",
    "# Remove ticks\n",
    "ins.tick_params(\n",
    "    axis='x',          # changes apply to the x-axis\n",
    "    which='both',      # both major and minor ticks are affected\n",
    "    bottom=False,      # ticks along the bottom edge are off\n",
    "    top=False,         # ticks along the top edge are off\n",
    "    labelbottom=False,\n",
    ")\n",
    "# Set ticks\n",
    "ymin = int(min(slopes)/0.1)*0.1-0.1\n",
    "ymax = int(max(slopes)/0.1)*0.1+0.1\n",
    "ins.set_yticks(np.arange(ymin, ymax, 0.1))\n",
    "ins.tick_params(axis='y', labelsize=12)\n",
    "ins.set_ylim(ymin, ymax)\n",
    "ins.set_ylabel('Trend\\n[m s$^{-1}$ decade$^{-1}$]', fontsize=12)\n",
    "\n",
    "# Labels\n",
    "ax.set_xlabel('Year')\n",
    "ax.set_ylabel('NSWS Anomaly [m s$^{-1}$]')\n",
    "ax.set_title('GSOD stations from Zeng et. al (2019)')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5fdd95c4",
   "metadata": {},
   "source": [
    "## Plots Average Land Trend\n",
    "\n",
    "Piecewise linear trends are calculated using `fit` from the `pwlf` package. P-values are determined using the `p_values()` function with `method='non-linear'`. From the docs: \n",
    "\n",
    "\"The non-linear regression problem is when you don’t know the breakpoint locations (e.g. when using the fit, fitfast, and fit_guess functions).\"\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52436d99",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Masking regions\n",
    "land_region = regionmask.defined_regions.natural_earth_v5_0_0.land_110  # Land has value 0\n",
    "countries = regionmask.defined_regions.natural_earth_v5_0_0.countries_110\n",
    "\n",
    "fig = plt.figure(figsize=(14, int(n_models*3.5)), constrained_layout=True)\n",
    "gs = fig.add_gridspec(n_models, 2, width_ratios=[1.5, 1])\n",
    "\n",
    "# Save data to dataframe\n",
    "piecewise_params = pd.DataFrame(\n",
    "    dict(\n",
    "        source='GSOD',\n",
    "        break_year=break_year,\n",
    "        slope_1=slopes[0],\n",
    "        slope_2=slopes[1],\n",
    "        p_val_1=p_vals[0],\n",
    "        p_val_2=p_vals[1],\n",
    "        sig_1=p_vals[0] < 0.01,\n",
    "        sig_2=p_vals[1] < 0.01,\n",
    "    ), index=[0]\n",
    ")\n",
    "\n",
    "for i, model in enumerate(tqdm(ensemble.keys(), desc='Plotting models')):\n",
    "    ds = ensemble[model]\n",
    "    # Get anomaly\n",
    "    ds = ds - ds.cf.mean('T')\n",
    "    # mask to land\n",
    "    land = mask_data(ds, land_region, ['land'], drop=True)\n",
    "    # Eliminate the planet of Hoth\n",
    "    land = mask_data(land, countries, ['greenland'], reverse=True, drop=True)\n",
    "    land = land.where((land['lat']>-59).compute(), drop=True)  # antarctica\n",
    "    land = land.where((land['lat']<70).compute(), drop=True)  # Northern canada\n",
    "    # land = land.where(land,drop=True)\n",
    "    # Create axes for plotting\n",
    "    map = fig.add_subplot(gs[i, 0], projection=ccrs.PlateCarree())\n",
    "    ts = fig.add_subplot(gs[i, 1])\n",
    "    # Plot map\n",
    "    trend = (\n",
    "        land['sfcWind'].cf.groupby('T.year').mean()\n",
    "        .polyfit('year', deg=1, skipna=True)\n",
    "        .polyfit_coefficients.sel(degree=1)*10  # decadal\n",
    "    )\n",
    "    im = trend.plot(ax=map, vmin=-0.2, vmax=0.2, cmap='coolwarm', transform=ccrs.PlateCarree(), add_colorbar=False)\n",
    "    cb = plt.colorbar(im, orientation=\"vertical\", pad=0.01, extend='both', shrink=0.7)\n",
    "    cb.set_label(label='Decadal Trend [m/s]')\n",
    "    # Plot time series\n",
    "    land_nsws = land['sfcWind'].cf.groupby('T.year').mean().cf.mean(['X', 'Y']).values\n",
    "    ts.plot(years, land_nsws, label=model, color='blue', linestyle='--')\n",
    "    ts.plot(years, obs_anom, label='GSOD', color='k', linestyle='--')\n",
    "    ts.plot(years, gsod_pwlf.predict(years), color='k')\n",
    "    # Add model linear fit\n",
    "    model_pwlf = pwlf.PiecewiseLinFit(years, land_nsws)\n",
    "    breaks = model_pwlf.fit(2)\n",
    "    break_year = int(breaks[1])\n",
    "    slopes = model_pwlf.calc_slopes()*10  # make decadal\n",
    "    p_vals = model_pwlf.p_values(method='non-linear')[1:]  # betas are [intercept, slope1, slope2, ...]\n",
    "    # print(f'Piecewise Parameters\\n\\tBreakpoint: {break_year}\\n\\tDecadal Slopes: {slopes}\\n\\tP-Values: {p_vals}')\n",
    "    ts.plot(years, model_pwlf.predict(years), color='blue')\n",
    "    ax_min = min(min(obs_anom), min(land_nsws)) - 0.05\n",
    "    ax_max = max(max(obs_anom), max(land_nsws)) + 0.05\n",
    "    ts.vlines(break_year, ax_min, ax_max, color='gray', linestyle='--')\n",
    "    ts.set_ylim(ax_min, ax_max)\n",
    "    # Annotation plot for trend\n",
    "    ins = ts.inset_axes([0.7, 0.6, 0.25, 0.35])\n",
    "    pos = slopes > 0\n",
    "    neg = slopes < 0\n",
    "    l = np.array([1, 2])\n",
    "    for i, (m, p) in enumerate(zip(slopes, p_vals)):\n",
    "        i+=1\n",
    "        if m > 0:\n",
    "            rects = ins.bar(i, m, color='red')\n",
    "        else:\n",
    "            rects = ins.bar(i, m, color='blue')\n",
    "        if p < 0.05:\n",
    "            ins.bar_label(rects, ['p$<$0.05'])\n",
    "    # Partition the percentile values to be able to draw large numbers in\n",
    "    # white within the bar, and small numbers in black outside the bar.\n",
    "    ins.hlines(0, 0.25, 2.75, color='k')\n",
    "    # Remove spines\n",
    "    ins.spines['top'].set_visible(False)\n",
    "    ins.spines['right'].set_visible(False)\n",
    "    ins.spines['bottom'].set_visible(False)\n",
    "    # Remove ticks\n",
    "    ins.tick_params(\n",
    "        axis='x',          # changes apply to the x-axis\n",
    "        which='both',      # both major and minor ticks are affected\n",
    "        bottom=False,      # ticks along the bottom edge are off\n",
    "        top=False,         # ticks along the top edge are off\n",
    "        labelbottom=False,\n",
    "    )\n",
    "    # Set ticks\n",
    "    ymin = int(min(slopes)/0.1)*0.1-0.1\n",
    "    ymax = int(max(slopes)/0.1)*0.1+0.1\n",
    "    ins.set_yticks(np.arange(ymin, ymax, 0.1))\n",
    "    ins.tick_params(axis='y', labelsize=10)\n",
    "    ins.set_ylim(ymin, ymax)\n",
    "    ins.set_ylabel('Trend\\n[m s$^{-1}$ decade$^{-1}$]', fontsize=10)\n",
    "    \n",
    "    \n",
    "    # Map options\n",
    "    map.add_feature(cfeature.COASTLINE)\n",
    "    # map.add_feature(cfeature.BORDERS)\n",
    "    map.set_title(f'{model} Decadal NSWS Trend')\n",
    "    gl = map.gridlines(\n",
    "        crs=ccrs.PlateCarree(), draw_labels=True, which='both',\n",
    "        linewidth=1, color='k', alpha=1, linestyle='--'\n",
    "    )\n",
    "    gl.right_labels = None\n",
    "    gl.top_labels = None\n",
    "    gl.xlines = None\n",
    "    gl.ylines = None\n",
    "    lon_formatter = LongitudeFormatter(zero_direction_label=True)\n",
    "    lat_formatter = LatitudeFormatter()\n",
    "    ax.xaxis.set_major_formatter(lon_formatter)\n",
    "    ax.yaxis.set_major_formatter(lat_formatter)\n",
    "    # Increase the ticksize\n",
    "    gl.xlabel_style = {'size': 12, 'color': 'k', 'rotation':30, 'ha':'right'}\n",
    "    gl.ylabel_style = {'size': 12, 'color': 'k', 'weight': 'normal'}\n",
    "\n",
    "    # Time series options\n",
    "    ts.set_title(f'{model} Land NSWS')\n",
    "    ts.set_ylabel('NSWS Anomaly [m s$^{-1}$]')\n",
    "    # Save results of piecewise fit\n",
    "    piecewise_params = pd.concat([\n",
    "        piecewise_params, \n",
    "        pd.DataFrame(\n",
    "            dict(\n",
    "                source=model,\n",
    "                break_year=break_year,\n",
    "                slope_1=slopes[0],\n",
    "                slope_2=slopes[1],\n",
    "                p_val_1=p_vals[0],\n",
    "                p_val_2=p_vals[1],\n",
    "                sig_1=p_vals[0] < 0.05,\n",
    "                sig_2=p_vals[1] < 0.05\n",
    "                ), index=[0]\n",
    "        )\n",
    "        ], ignore_index=True)\n",
    "\n",
    "plt.savefig('figures/amip_land_trends.png', dpi=300, bbox_inches='tight', pad_inches=0.1)\n",
    "print('Parameters of piecewise regression:')\n",
    "print(piecewise_params)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29563edd",
   "metadata": {},
   "source": [
    "## Heatmap of prediction\n",
    "\n",
    "Using Giorgi scientific regions (from Giorgi and Franciso, 2000). From regionmask:\n",
    "\n",
    "\"The Giorgi reference regions, rectangular regions proposed in Giorgi and Francisco, 2000 were used in the third and fourth assessment reports of the Intergovernmental Panel on Climate Change (IPCC).\"\n",
    "\n",
    "For observation data, all regions with <40 stations are ignored. This is to ensure reliable consistency\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c33864c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Masking regions\n",
    "giorgi = regionmask.defined_regions.giorgi\n",
    "land_region = regionmask.defined_regions.natural_earth_v5_0_0.land_110  # Land has value 0\n",
    "regions = giorgi.abbrevs\n",
    "regions.remove('GRL')  # don't do greenland\n",
    "\n",
    "# Get trends for observational data\n",
    "regressed = dict(source='Observation')\n",
    "bad_regions = []\n",
    "# Add global trends to dataframe\n",
    "glob_trend = obs_anom.polyfit('year', deg=1, skipna=True).polyfit_coefficients.sel(degree=1)*10\n",
    "regressed['Global'] = glob_trend.values\n",
    "obs_mask = giorgi.mask(gsod.lon, gsod.lat)\n",
    "for r in regions:\n",
    "    # Extract keys for the region\n",
    "    key = giorgi.map_keys(r)\n",
    "    obs = gsod.where(obs_mask == key, drop=True)\n",
    "    obs = obs - obs.cf.mean('year')\n",
    "    if obs.station.size < 40:\n",
    "        bad_regions.append(r)\n",
    "    else:\n",
    "        decade_trend = obs.GSOD.mean('station').polyfit('year', deg=1, skipna=True).polyfit_coefficients.sel(degree=1)*10\n",
    "        regressed[r] = decade_trend.values\n",
    "# Create dataframe to store regressions\n",
    "regions = [r for r in regions if r not in bad_regions]\n",
    "region_trends = pd.DataFrame(\n",
    "    columns=['source', *regions]\n",
    ")\n",
    "# Add GSOD data\n",
    "region_trends = pd.concat(\n",
    "    [\n",
    "        region_trends,\n",
    "        pd.DataFrame(regressed, index=[0])\n",
    "    ], ignore_index=True\n",
    ")\n",
    "    \n",
    "for i, model in enumerate(tqdm(ensemble.keys(), desc='Deriving Regional Trends')):\n",
    "    ds = ensemble[model]\n",
    "    # Get anomaly\n",
    "    ds = ds - ds.cf.mean('T')\n",
    "    # Resample to yearly\n",
    "    da = ds['sfcWind'].cf.groupby('T.year').mean()\n",
    "    # mask to land\n",
    "    land = mask_data(da, land_region, ['land'], drop=True)\n",
    "    # Eliminate the planet of Hoth\n",
    "    land = land.where((land['lat']>-59).compute(), drop=True)  # antarctica\n",
    "    land = land.where((land['lat']<70).compute(), drop=True)  # Northern canada\n",
    "    # For each region determine the average trend\n",
    "    regressed = dict(source=model)\n",
    "    # Add Global trend\n",
    "    glob_trend = land.cf.mean(['X', 'Y']).polyfit('year', deg=1, skipna=True).polyfit_coefficients.sel(degree=1)*10\n",
    "    regressed['Global'] = glob_trend.values\n",
    "    for r in regions:\n",
    "        masked = mask_data(land, giorgi, [r], drop=True)\n",
    "        decade_trend = masked.cf.mean(['X', 'Y']).polyfit('year', deg=1, skipna=True).polyfit_coefficients.sel(degree=1)*10\n",
    "        regressed[r] = decade_trend.values    \n",
    "    # Add to dataframe\n",
    "    region_trends = pd.concat(\n",
    "        [\n",
    "            region_trends,\n",
    "            pd.DataFrame(regressed, index=[0])\n",
    "        ], ignore_index=True\n",
    "    )\n",
    "region_trends"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9634bb0",
   "metadata": {},
   "source": [
    "### Heatmap of prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1fddc6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "x = 12\n",
    "y = 12\n",
    "columns = [*regions, 'Global']\n",
    "fig, axes = plt.subplots(figsize=(x, y), nrows=2)\n",
    "fig.subplots_adjust(hspace=.4)\n",
    "data = region_trends[columns].T\n",
    "# Plot 1: trend\n",
    "ax = axes.flat[0]\n",
    "im = ax.pcolor(data, cmap='bwr', vmin=-0.15, vmax=0.15)\n",
    "# Add colorbar\n",
    "cb = plt.colorbar(im, orientation='vertical', pad=0.01, extend='both')\n",
    "cb.set_label(label='Decadal Trend [m s$^{-1}$ decade$^{-1}$]', fontsize=14)\n",
    "\n",
    "# Style options\n",
    "ax.set_ylabel('Giorgi Region', fontsize=14)\n",
    "ax.set_title('NSWS Wind Trends', fontsize=16)\n",
    "\n",
    "# Plot 2: Aggreement with observation\n",
    "ax = axes.flat[1]\n",
    "signs = np.copysign(np.ones_like(data), (data)).to_numpy()\n",
    "obs_sign = signs[:, 0]\n",
    "obs_sign = signs[:, 0].reshape(-1, 1)\n",
    "agreements = np.where(signs == obs_sign, 1, 0)\n",
    "im = ax.pcolor(agreements, cmap=colors.ListedColormap(['white', 'Green']))\n",
    "cb = plt.colorbar(im, orientation='vertical', pad=0.01, ticks=[0.25, 0.75])\n",
    "cb.set_ticklabels(['Disagree', 'Agree'], rotation=60)\n",
    "cb.set_label(label='Sign agreement with observation', fontsize=14)\n",
    "\n",
    "# Options for both plots\n",
    "for ax in axes.flat:\n",
    "    # set side axis\n",
    "    ax.set_ylim(ax.get_ylim())\n",
    "    ax.set_yticks(np.arange(len(columns))+0.5)\n",
    "    ax.set_yticklabels(columns, fontsize=10)\n",
    "    \n",
    "    # set top axis\n",
    "    ax.set_xlim(ax.get_xlim())\n",
    "    ax.set_xticks(np.arange(len(region_trends['source']))+0.5)\n",
    "    ax.set_xticklabels(region_trends['source'], rotation=90, fontsize=10)\n",
    "    \n",
    "    # For grid lines\n",
    "    ax.set_yticks(np.arange(0, len(columns)), minor=True)\n",
    "    ax.set_xticks(np.arange(0, len(region_trends['source'])), minor=True)\n",
    "    ax.grid(True, axis='both', which='minor', color='k', linewidth=2)\n",
    "    ax.tick_params(axis='both', which='major', length=1)\n",
    "    \n",
    "    # Highlighted models\n",
    "    for i in [4, 5]:  \n",
    "        label = ax.xaxis.get_ticklabels()[i]\n",
    "        label.set_bbox(dict(facecolor='none', edgecolor='red'))\n",
    "\n",
    "plt.savefig('figures/amip_regional_trends.png', dpi=300, bbox_inches='tight', pad_inches=0.1)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47d732a5",
   "metadata": {},
   "source": [
    "## Investigate best models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b45adc11",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(8, 6))\n",
    "\n",
    "ax.plot(years, obs_anom, label='GSOD', color='k', linestyle='--')\n",
    "ax.plot(years, gsod_pwlf.predict(years), color='k')\n",
    "for model, color in zip(['CESM2', 'CESM2-WACCM'], ['blue', 'red']):\n",
    "    ds = ensemble[model]\n",
    "    # Get anomaly\n",
    "    ds = ds - ds.cf.mean('T')\n",
    "    # Resample to yearly\n",
    "    da = ds['sfcWind'].cf.groupby('T.year').mean()\n",
    "    # mask to land\n",
    "    land = mask_data(da, land_region, ['land'], drop=True)\n",
    "    # Eliminate the planet of Hoth\n",
    "    land = land.where((land['lat']>-59).compute(), drop=True)  # antarctica\n",
    "    land = land.where((land['lat']<70).compute(), drop=True)  # Northern canada\n",
    "    # Plot time series\n",
    "    land_nsws = land.cf.mean(['X', 'Y']).values\n",
    "    ax.plot(years, land_nsws, label=model, linestyle='--', color=color)\n",
    "    # Add model linear fit\n",
    "    model_pwlf = pwlf.PiecewiseLinFit(years, land_nsws)\n",
    "    breaks = model_pwlf.fit(2)\n",
    "    break_year = int(breaks[1])\n",
    "    slopes = model_pwlf.calc_slopes()*10  # make decadal\n",
    "    p_vals = model_pwlf.p_values(method='non-linear')[1:]  # betas are [intercept, slope1, slope2, ...]\n",
    "    # print(f'Piecewise Parameters\\n\\tBreakpoint: {break_year}\\n\\tDecadal Slopes: {slopes}\\n\\tP-Values: {p_vals}')\n",
    "    ax.plot(years, model_pwlf.predict(years), color=color)\n",
    "    ax_min = min(min(obs_anom), min(land_nsws)) - 0.05\n",
    "    ax_max = max(max(obs_anom), max(land_nsws)) + 0.05\n",
    "    # ax.vlines(break_year, ax_min, ax_max, color='gray', linestyle='--')\n",
    "    ax.set_ylim(ax_min, ax_max)\n",
    "    \n",
    "ax.legend()\n",
    "ax.set_xlabel('Year')\n",
    "ax.set_ylabel('NSWS Anomaly [m s$^{-1}$]')\n",
    "ax.set_title('Comparison of Best Models')\n",
    "\n",
    "plt.savefig('figures/amip_best_models.png', dpi=300, bbox_inches='tight', pad_inches=0.1)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b215423a",
   "metadata": {},
   "source": [
    "## Ocean Trend"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f28ecc0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Masking regions\n",
    "land_region = regionmask.defined_regions.natural_earth_v5_0_0.land_110  # Land has value 0\n",
    "countries = regionmask.defined_regions.natural_earth_v5_0_0.countries_110\n",
    "\n",
    "fig = plt.figure(figsize=(14, int(n_models*3.5)), constrained_layout=True)\n",
    "gs = fig.add_gridspec(n_models, 2, width_ratios=[1.5, 1])\n",
    "\n",
    "ocean_piecewise_params = pd.DataFrame(\n",
    "    columns = [\n",
    "        'source',\n",
    "        'break_year',\n",
    "        'slope_1',\n",
    "        'slope_2',\n",
    "        'p_val_1',\n",
    "        'p_val_2',\n",
    "        'sig_1',\n",
    "        'sig_2'\n",
    "    ]\n",
    ")\n",
    "\n",
    "for i, model in enumerate(tqdm(ensemble.keys(), desc='Plotting models')):\n",
    "    ds = ensemble[model]\n",
    "    # Get anomaly\n",
    "    ds = ds - ds.cf.mean('T')\n",
    "    # mask to land\n",
    "    ocean = mask_data(ds, land_region, ['land'], drop=True, reverse=True)\n",
    "    # Create axes for plotting\n",
    "    map = fig.add_subplot(gs[i, 0], projection=ccrs.PlateCarree())\n",
    "    ts = fig.add_subplot(gs[i, 1])\n",
    "    # Plot map\n",
    "    trend = (\n",
    "        ocean['sfcWind'].cf.groupby('T.year').mean()\n",
    "        .polyfit('year', deg=1, skipna=True)\n",
    "        .polyfit_coefficients.sel(degree=1)*10  # decadal\n",
    "    )\n",
    "    im = trend.plot(ax=map, vmin=-0.2, vmax=0.2, cmap='coolwarm', transform=ccrs.PlateCarree(), add_colorbar=False)\n",
    "    cb = plt.colorbar(im, orientation=\"vertical\", pad=0.01, extend='both', shrink=0.7)\n",
    "    cb.set_label(label='Decadal Trend [m/s]')\n",
    "    # Plot time series\n",
    "    ocean_nsws = ocean['sfcWind'].cf.groupby('T.year').mean().cf.mean(['X', 'Y']).values\n",
    "    ts.plot(years, ocean_nsws, label=model, color='blue', linestyle='--')\n",
    "    # Add model linear fit\n",
    "    model_pwlf = pwlf.PiecewiseLinFit(years, ocean_nsws)\n",
    "    breaks = model_pwlf.fit(2)\n",
    "    break_year = int(breaks[1])\n",
    "    slopes = model_pwlf.calc_slopes()*10  # make decadal\n",
    "    p_vals = model_pwlf.p_values(method='non-linear')[1:]  # betas are [intercept, slope1, slope2, ...]\n",
    "    # print(f'Piecewise Parameters\\n\\tBreakpoint: {break_year}\\n\\tDecadal Slopes: {slopes}\\n\\tP-Values: {p_vals}')\n",
    "    ts.plot(years, model_pwlf.predict(years), color='blue')\n",
    "    ax_min = min(min(obs_anom), min(ocean_nsws)) - 0.05\n",
    "    ax_max = max(max(obs_anom), max(ocean_nsws)) + 0.05\n",
    "    ts.vlines(break_year, ax_min, ax_max, color='gray', linestyle='--')\n",
    "    ts.set_ylim(ax_min, ax_max)\n",
    "    # Annotation plot for trend\n",
    "    ins = ts.inset_axes([0.7, 0.6, 0.25, 0.35])\n",
    "    pos = slopes > 0\n",
    "    neg = slopes < 0\n",
    "    l = np.array([1, 2])\n",
    "    for i, (m, p) in enumerate(zip(slopes, p_vals)):\n",
    "        i+=1\n",
    "        if m > 0:\n",
    "            rects = ins.bar(i, m, color='red')\n",
    "        else:\n",
    "            rects = ins.bar(i, m, color='blue')\n",
    "        if p < 0.05:\n",
    "            ins.bar_label(rects, ['p$<$0.05'])\n",
    "    # Partition the percentile values to be able to draw large numbers in\n",
    "    # white within the bar, and small numbers in black outside the bar.\n",
    "    ins.hlines(0, 0.25, 2.75, color='k')\n",
    "    # Remove spines\n",
    "    ins.spines['top'].set_visible(False)\n",
    "    ins.spines['right'].set_visible(False)\n",
    "    ins.spines['bottom'].set_visible(False)\n",
    "    # Remove ticks\n",
    "    ins.tick_params(\n",
    "        axis='x',          # changes apply to the x-axis\n",
    "        which='both',      # both major and minor ticks are affected\n",
    "        bottom=False,      # ticks along the bottom edge are off\n",
    "        top=False,         # ticks along the top edge are off\n",
    "        labelbottom=False,\n",
    "    )\n",
    "    # Set ticks\n",
    "    ymin = int(min(slopes)/0.1)*0.1-0.1\n",
    "    ymax = int(max(slopes)/0.1)*0.1+0.1\n",
    "    ins.set_yticks(np.arange(ymin, ymax, 0.1))\n",
    "    ins.tick_params(axis='y', labelsize=10)\n",
    "    ins.set_ylim(ymin, ymax)\n",
    "    ins.set_ylabel('Trend\\n[m s$^{-1}$ decade$^{-1}$]', fontsize=10)\n",
    "    \n",
    "    \n",
    "    # Map options\n",
    "    map.add_feature(cfeature.COASTLINE)\n",
    "    # map.add_feature(cfeature.BORDERS)\n",
    "    map.set_title(f'{model} Decadal NSWS Trend')\n",
    "    gl = map.gridlines(\n",
    "        crs=ccrs.PlateCarree(), draw_labels=True, which='both',\n",
    "        linewidth=1, color='k', alpha=1, linestyle='--'\n",
    "    )\n",
    "    gl.right_labels = None\n",
    "    gl.top_labels = None\n",
    "    gl.xlines = None\n",
    "    gl.ylines = None\n",
    "    lon_formatter = LongitudeFormatter(zero_direction_label=True)\n",
    "    lat_formatter = LatitudeFormatter()\n",
    "    ax.xaxis.set_major_formatter(lon_formatter)\n",
    "    ax.yaxis.set_major_formatter(lat_formatter)\n",
    "    # Increase the ticksize\n",
    "    gl.xlabel_style = {'size': 12, 'color': 'k', 'rotation':30, 'ha':'right'}\n",
    "    gl.ylabel_style = {'size': 12, 'color': 'k', 'weight': 'normal'}\n",
    "\n",
    "    # Time series options\n",
    "    ts.set_title(f'{model} Ocean NSWS')\n",
    "    ts.set_ylabel('NSWS Anomaly [m s$^{-1}$]')\n",
    "    # Save results of piecewise fit\n",
    "    ocean_piecewise_params = pd.concat([\n",
    "        ocean_piecewise_params, \n",
    "        pd.DataFrame(\n",
    "            dict(\n",
    "                source=model,\n",
    "                break_year=break_year,\n",
    "                slope_1=slopes[0],\n",
    "                slope_2=slopes[1],\n",
    "                p_val_1=p_vals[0],\n",
    "                p_val_2=p_vals[1],\n",
    "                sig_1=p_vals[0] < 0.05,\n",
    "                sig_2=p_vals[1] < 0.05\n",
    "                ), index=[0]\n",
    "        )\n",
    "        ], ignore_index=True)\n",
    "\n",
    "plt.savefig('figures/amip_ocean_trends.png', dpi=300, bbox_inches='tight', pad_inches=0.1)\n",
    "# print('Parameters of piecewise regression:')\n",
    "print(ocean_piecewise_params)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff44d4d5",
   "metadata": {},
   "source": [
    "## Kill Dask cluster"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19a41c4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "client.shutdown()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "xcdat",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
